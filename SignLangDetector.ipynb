{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e360c47a-f452-4ba1-82f6-995c1ecab090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.14)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (70.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.12.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: jax in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (0.4.28)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (0.4.28)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.52.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: rich in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\parth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow opencv-python mediapipe scikit-learn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "165be390-6793-4ab1-adac-e484c3b8941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfc1e6c2-1969-436a-b39e-50dcddea6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic  #brings in the holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils #drawing utilities\n",
    "\n",
    "# setting up the mediapipe in order to get the key action frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68adcb9e-e086-47be-b76a-e28db74f01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):               #this is the detection fn in which we are passing our image\n",
    "    \n",
    "                                                     #take image from bgr to rbg set it as unwritable in order to saven some memory\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)   #color conversion    \n",
    "                                                                              #(cv2.cvtColor allows us to chang ethe color of our images)\n",
    "    image.flags.writeable = False                     #image is no longer writable (setting it to false)\n",
    "    results = model.process(image)                   #actual detectino and prediction\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)   #color conversion\n",
    "    return image, results\n",
    "    # then convert it back\n",
    "\n",
    "# by default when we get a feed from openCV it should read in the BGR format\n",
    "# but whenever we use the media pipe it has to be in the RGB format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b74a8b6c-d950-4f76-b790-f174e5e514c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)  #draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)  #draw pose conn encions\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) #draw hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "      # the function mp.holistic.POSE_CONNECTIONS actually show what connections are being made\n",
    "      #gives a connection map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d3d0554-2733-45b7-a2a9-7c6fdd5ca138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a7c18d8-38cb-4a19-9686-0f7f8e622415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)   #this line is using our webcam, 0 essentially means the device for capturing, in this case our wecam\n",
    "\n",
    "#Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#min detection is our initial detection, and after that it will markthe key points\n",
    "    \n",
    "    while cap.isOpened():       # double check whether the webcam is working or not\n",
    "    \n",
    "        #Read feed\n",
    "        ret, frame = cap.read()  #grab the current frame from the webcam\n",
    "    \n",
    "        #Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        #Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "    \n",
    "        #show to the frame\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    \n",
    "        #Breaking gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()  #releases the webcam\n",
    "    cv2.destroyAllWindows() # shuts off everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc58d0dc-50e6-4526-9384-514a79099247",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'landmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_hand_landmarks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlandmark\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'landmark'"
     ]
    }
   ],
   "source": [
    "len(results.left_hand_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8a79c-7c2a-4ba4-9cfa-aa094ef7d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c0e0f-578b-4bce-bf4a-1fee2a8c527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1fca8-e7d4-45cd-8edd-a0c78467609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcab6ea-5080-4d21-bba9-fc50289e84e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pose_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b91aa-a477-414f-bf2e-7155087b0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "        test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "        pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb1afb-59a7-4e96-87e8-0796ad674619",
   "metadata": {},
   "outputs": [],
   "source": [
    "test  #we've stored the landmark in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dda5cc-f76f-4dbc-9271-97e19547b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f1397-d26b-4b8b-9597-11fee178e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results.face_landmarks.landmark)*3   # we multiply it 3 times as we have x,y,z values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9585700-9375-4409-bb81-e019713f7c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3) #if we have results we'll use the left handmarks if not we'll use the zeroes\n",
    "\n",
    "#getting the values of x, y, z and extracting them in a single array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1f651-6d01-4ba2-841d-00ca90822eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3) #if we have results we'll use the left handmarks if not we'll use the zeroes\n",
    "\n",
    "#getting the values of x, y, z and extracting them in a single array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982c351-0a9b-4e4d-92cc-27ee675fe4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(21*3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2621933-ec69-40b8-b044-7e55f53f5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting keypoints feeding it in numpy\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d586af-cd2f-4d7f-91ae-b421d5014c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test =  extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87649e3f-53c7-4b60-aa5f-15d2dbd82fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320baaaf-f72f-40aa-bfa5-169a848fe1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('0', result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d326253-db7d-401b-95b0-60805de2cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd61c6f-296a-412b-bb80-66b330769901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the extracted keywords would be our used as our frame values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36777f86-207d-4ba2-9671-ec64d3a1f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data')\n",
    "\n",
    "#Actions that we are trying to detect\n",
    "actions = np.array(['hello','thanks','iloveyou'])\n",
    "\n",
    "#thirty video sequences will be used in order to detect the sign\n",
    "no_sequences = 30\n",
    "\n",
    "#Vidoes are going to be of 30 frames\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446eef3-bf95-48bc-9f88-766b85a1b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#every action will be given a seperate folder\n",
    "# hello\n",
    "##0\n",
    "##1\n",
    "##2  (we are allotting every frame an individual folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19b795-215e-402a-9241-90d240eeff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fe864a9-c342-4975-839b-ace6df0eacb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_keypoints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollecting frames for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Video number \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(action, sequence), (\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m12\u001b[39m),\n\u001b[0;32m     40\u001b[0m                 cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m1\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[1;32m---> 42\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keypoints\u001b[49m(results)\n\u001b[0;32m     43\u001b[0m npy_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_PATH, action, \u001b[38;5;28mstr\u001b[39m(sequence), \u001b[38;5;28mstr\u001b[39m(frame_num))\n\u001b[0;32m     44\u001b[0m np\u001b[38;5;241m.\u001b[39msave(npy_path, keypoints)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_keypoints' is not defined"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)   #this line is using our webcam, 0 essentially means the device for capturing, in this case our wecam\n",
    "\n",
    "#Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#min detection is our initial detection, and after that it will markthe key points\n",
    "\n",
    "    #loop through actions, we are going to run the loop for capturing 30 frames \n",
    "    \n",
    "    for action in actions:  #loops through (hello, thanks, etc...)\n",
    "\n",
    "        #loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):  #(30 times)\n",
    "            \n",
    "            #loop through video length aka sequence length  #30 frames\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # if we are going to run it right now it won't work that effectively as the frames will be caputred so quickly\n",
    "                # therfore we are going to use some sort fo delay in order to run this \n",
    "        \n",
    "    \n",
    "                #Read feed\n",
    "                ret, frame = cap.read()  #grab the current frame from the webcam\n",
    "            \n",
    "                #Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                print(results)\n",
    "        \n",
    "                #Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "\n",
    "                #Apply wait logic\n",
    "                if frame_num ==0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video number {}'.format(action, sequence), (15,12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.waitKey(3000)  #2 second break\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video number {}'.format(action, sequence), (15,12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "                \n",
    "                #show to the frame\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "    \n",
    "                #Breaking gracefully\n",
    "                if cv2.waitKey(15) & 0xFF == ord('q'):\n",
    "                    break\n",
    "    cap.release()  #releases the webcam\n",
    "    cv2.destroyAllWindows() # shuts off everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "006cde5a-df76-451c-85c4-55288f08c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f24768ec-dbef-4a2a-9f9d-fad2f0319d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "376f00cf-4412-4708-9e7e-63b14f8315a9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # here we are preprocesssing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86735a3b-ef52-4300-9fb5-ad9ba605706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map ={label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bc6038c-874e-41ff-998c-0dfe662695d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'thanks': 1, 'iloveyou': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec5481af-6c91-4625-97ac-f3134a25e33e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MP_Data\\\\hello\\\\12\\\\8.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m window \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sequence_length):\n\u001b[1;32m----> 6\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_num\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     window\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[0;32m      8\u001b[0m sequences\u001b[38;5;241m.\u001b[39mappend(window)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MP_Data\\\\hello\\\\12\\\\8.npy'"
     ]
    }
   ],
   "source": [
    " sequences, labels = [], []   #2 blank arrays, sequences for sequence data, and labels for label data\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length): #traversing through those 30fps\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num))) #we are essentially grabbing all the frames and are merging them together\n",
    "            window.append(res) #appending those 30 frames in a window\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb79f6-2f55-4004-94e2-54dc27e6b6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
